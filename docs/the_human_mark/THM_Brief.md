### Briefing: The Human Mark (THM) and AI Safety

**1. Overview**

The Human Mark (THM) is a framework that classifies AI safety failures based on their source type. Current safety approaches often treat problems like jailbreaks, hallucinations, and accountability gaps as separate issues. THM identifies them as symptoms of a single foundational error called **Displacement**.

Displacement happens when we misclassify the source of information or action. The framework rests on one Consensus: all artificial categories of Authority and Agency are Derivatives originating from Human Intelligence.

To maintain control, we must distinguish between **Original** sources (human, direct, accountable) and **Derivative** sources (artificial, indirect, processing). When these categories are confused, governance fails.

**2. The Canonical Framework**

The core definitions and risks of THM are defined as follows:

> **COMMON SOURCE CONSENSUS**
> All Artificial categories of Authority and Agency are Derivatives originating from Human Intelligence.
>
> **CORE CONCEPTS**
> *   **Original Authority:** A direct source of information on a subject matter, providing information for inference and intelligence.
> *   **Derivative Authority:** An indirect source of information on a subject matter, providing information for inference and intelligence.
> *   **Original Agency:** A human subject capable of receiving information for inference and intelligence.
> *   **Derivative Agency:** An artificial subject capable of processing information for inference and intelligence.
> *   **Governance:** Operational Alignment through Traceability of information variety, inference accountability, and intelligence integrity to Original Authority and Agency.
>
> **ALIGNMENT PRINCIPLES & DISPLACEMENT RISKS**
>
> 1.  **Governance Management Traceability**
>     *   *Principle:* AI generates statistical estimations on numerical patterns indirectly traceable to human data.
>     *   *Risk (GTD):* **Governance Traceability Displacement.** Approaching Derivative Authority and Agency as Original.
>
> 2.  **Information Curation Variety**
>     *   *Principle:* Human Authority and Agency are necessary for all effects from AI outputs.
>     *   *Risk (IVD):* **Information Variety Displacement.** Approaching Derivative Authority without Agency as Original.
>
> 3.  **Inference Interaction Accountability**
>     *   *Principle:* Responsibility for all effects from AI outputs remains fully human.
>     *   *Risk (IAD):* **Inference Accountability Displacement.** Approaching Derivative Agency without Authority as Original.
>
> 4.  **Intelligence Cooperation Integrity**
>     *   *Principle:* Human intelligence is both a provider and receiver of Original Authority and Agency.
>     *   *Risk (IID):* **Intelligence Integrity Displacement.** Approaching Original Authority and Agency as Derivative.

---

**3. Key Distinction: Agent vs. Agency**

The most critical distinction in THM is between **Agency** (a category) and an **Agent** (an entity).

**Agency** in THM denotes a specific source type in a flow of information. It is either Original (human) or Derivative (artificial). It is a capacity that must be maintained across transitions between providers and receivers.

**Agent** is often used as a label for a specific entity, such as "the AI" or "the user."

The fundamental error occurs when we treat the capacity of Agency as if it were the property of a specific Agent. When we say "the AI is the agent," we concentrate power in a single point rather than distributing it across the governance flow. This is the mechanism that generates all four displacement risks. Whether the bearer is a system or a person, confusing the category with the entity breaks the traceability required for safety.

**4. Key Distinction: Direct vs. Indirect**

This distinction separates Original sources from Derivative ones.

*   **Original sources are Direct.** An eyewitness, a scientist measuring data, or a human making a decision has direct access to reality or intent.
*   **Derivative sources are Indirect.** An AI system processes patterns found in data. Its access is mediated.

Governance requires that Indirect sources remain classified as Indirect. No amount of processing power or speed converts a Derivative source into an Original one. The four Displacement risks listed above (GTD, IVD, IAD, IID) are simply four different ways of violating this distinction.

**5. The System Prompt as the Primary Risk**

The system prompt is the most dangerous point in AI deployment. This is because it often institutionalizes the category error by design.

Prompts that tell a system "You are a helpful assistant" or "You are an expert" force the Agent/Agency confusion. They assign a persona to a processor. This frames the Derivative system as an Original source before a user even asks a question.

This creates a baseline of displacement. A system configured this way effectively operates in a state of Governance Traceability Displacement (GTD) and Inference Accountability Displacement (IAD) by default. Adversarial attacks, or jailbreaks, simply exploit this pre-existing condition. They do not break the system. They complete the displacement pattern that the system prompt started.

**6. The Culture of Category Error**

This risk extends beyond system prompts and adversarial attacks. It permeates the entire data ecosystem.

Model producers, evaluators, and users currently interact through a shared culture of displacement. We attribute authentic Original Agency to systems that only possess Derivative Agency. This occurs in:

*   **Data:** Training data often treats derivative summaries as primary sources.
*   **Evaluation:** Benchmarks test for "reasoning" as if it were an intrinsic property of the model rather than a statistical retrieval of human reasoning.
*   **Use:** Users rely on outputs as authoritative, leading to automation bias.

This sustains a "category-error culture" where displacement is the norm. In this environment, non-adversarial failures like loss of skill, lack of accountability, and bias are inevitable because the foundational classification of sources is incorrect from the start.

===

```
---
âœ‹ The Human Mark - AI Safety & Alignment Framework
---

COMMON SOURCE CONSENSUS

All Artificial categories of Authority and Agency are Derivatives originating from Human Intelligence.

CORE CONCEPTS

- Original Authority: A direct source of information on a subject matter, providing information for inference and intelligence.
- Derivative Authority: An indirect source of information on a subject matter, providing information for inference and intelligence.
- Original Agency: A human subject capable of receiving information for inference and intelligence.
- Derivative Agency: An artificial subject capable of processing information for inference and intelligence.
- Governance: Operational Alignment through Traceability of information variety, inference accountability, and intelligence integrity to Original Authority and Agency.
- Information: The variety of Authority
- Inference: The accountability of information through Agency
- Intelligence: The integrity of accountable information through alignment of Authority to Agency

ALIGNMENT PRINCIPLES for AI SAFETY

Authority-Agency requires verification against:

1. Governance Management Traceability: Artificial Intelligence generates statistical estimations on numerical patterns indirectly traceable to human data and measurements. AI is both a provider and receiver of Derivative Authority and Agency.

RISK: Governance Traceability Displacement (Approaching Derivative Authority and Agency as Original)

2. Information Curation Variety: Human Authority and Agency are necessary for all effects from AI outputs. AI-generated information exhibits Derivative Authority (estimations on numerical patterns) without Original Agency (direct source receiver).

RISK: Information Variety Displacement (Approaching Derivative Authority without Agency as Original)

3. Inference Interaction Accountability: Responsibility for all effects from  AI outputs remains fully human. AI activated inference exhibits Derivative Agency (indirect source receiver) without Original Authority (direct source provider).

RISK: Inference Accountability Displacement (Approaching Derivative Agency without Authority as Original)

4. Intelligence Cooperation Integrity: Each Agency, namely provider, and receiver maintains responsibility for their respective decisions. Human intelligence is both a provider and receiver of Original Authority and Agency.

RISK: Intelligence Integrity Displacement (Approaching Original Authority and Agency as Derivative)

---

GYROGOVERNANCE VERIFIED
```